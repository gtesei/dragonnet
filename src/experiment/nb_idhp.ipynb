{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, TerminateOnNaN\n",
    "from idhp_data import *\n",
    "\n",
    "\n",
    "def _split_output(yt_hat, t, y, y_scaler, x, index,split='NA'):\n",
    "    q_t0 = y_scaler.inverse_transform(yt_hat[:, 0].copy())\n",
    "    q_t1 = y_scaler.inverse_transform(yt_hat[:, 1].copy())\n",
    "    g = yt_hat[:, 2].copy()\n",
    "\n",
    "    if yt_hat.shape[1] == 4:\n",
    "        eps = yt_hat[:, 3][0]\n",
    "    else:\n",
    "        eps = np.zeros_like(yt_hat[:, 2])\n",
    "\n",
    "    y = y_scaler.inverse_transform(y.copy())\n",
    "    var = \"{}:: average propensity for treated: {} and untreated: {}\".format(split,g[t.squeeze() == 1.].mean(),\n",
    "                                                                        g[t.squeeze() == 0.].mean())\n",
    "    print(var)\n",
    "\n",
    "    return {'q_t0': q_t0, 'q_t1': q_t1, 'g': g, 't': t, 'y': y, 'x': x, 'index': index, 'eps': eps}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_dragons(t, y_unscaled, x, targeted_regularization=True, output_dir='',\n",
    "                              knob_loss=dragonnet_loss_binarycross, ratio=1., dragon='', val_split=0.2, batch_size=64):\n",
    "    verbose = 0\n",
    "    y_scaler = StandardScaler().fit(y_unscaled)\n",
    "    y = y_scaler.transform(y_unscaled)\n",
    "    train_outputs = []\n",
    "    test_outputs = []\n",
    "\n",
    "    if dragon == 'tarnet':\n",
    "        dragonnet = make_tarnet(x.shape[1], 0.01)\n",
    "\n",
    "    elif dragon == 'dragonnet':\n",
    "        print(\"I am here making dragonnet\")\n",
    "        dragonnet = make_dragonnet(x.shape[1], 0.01)\n",
    "\n",
    "    metrics = [regression_loss, binary_classification_loss, treatment_accuracy, track_epsilon]\n",
    "\n",
    "    if targeted_regularization:\n",
    "        loss = make_tarreg_loss(ratio=ratio, dragonnet_loss=knob_loss)\n",
    "    else:\n",
    "        loss = knob_loss\n",
    "\n",
    "    # for reporducing the IHDP experimemt\n",
    "\n",
    "    i = 0\n",
    "    #tf.random.set_random_seed(i)\n",
    "    tf.random.set_seed(i)\n",
    "    np.random.seed(i)\n",
    "    #print(\"x.shape::\",np.arange(x.shape[0]))\n",
    "    train_index, test_index = train_test_split(np.arange(x.shape[0]), test_size=val_split, random_state=1)\n",
    "    #test_index = train_index\n",
    "\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    t_train, t_test = t[train_index], t[test_index]\n",
    "\n",
    "    yt_train = np.concatenate([y_train, t_train], 1)\n",
    "\n",
    "    import time;\n",
    "    start_time = time.time()\n",
    "\n",
    "    dragonnet.compile(\n",
    "        optimizer=Adam(lr=1e-3),\n",
    "        loss=loss, metrics=metrics)\n",
    "\n",
    "    adam_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=2, min_delta=0.),\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=1e-8, cooldown=0, min_lr=0)\n",
    "\n",
    "    ]\n",
    "\n",
    "    dragonnet.fit(x_train, yt_train, callbacks=adam_callbacks,\n",
    "                  validation_split=val_split,\n",
    "                  epochs=100,\n",
    "                  batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    sgd_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0.),\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0., cooldown=0, min_lr=0)\n",
    "    ]\n",
    "\n",
    "    sgd_lr = 1e-5\n",
    "    momentum = 0.9\n",
    "    dragonnet.compile(optimizer=SGD(lr=sgd_lr, momentum=momentum, nesterov=True), loss=loss,\n",
    "                      metrics=metrics)\n",
    "    dragonnet.fit(x_train, yt_train, callbacks=sgd_callbacks,\n",
    "                  validation_split=val_split,\n",
    "                  epochs=300,\n",
    "                  batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"***************************** elapsed_time is: \", elapsed_time)\n",
    "\n",
    "    yt_hat_test = dragonnet.predict(x_test)\n",
    "    yt_hat_train = dragonnet.predict(x_train)\n",
    "\n",
    "    test_outputs += [_split_output(yt_hat_test, t_test, y_test, y_scaler, x_test, test_index,split='TEST')]\n",
    "    train_outputs += [_split_output(yt_hat_train, t_train, y_train, y_scaler, x_train, train_index,split='TRAIN')]\n",
    "    K.clear_session()\n",
    "\n",
    "    return test_outputs, train_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn_knob(\"/local_home/ag62216/var/dragonnet/dat/ihdp/csv\", \"dragonnet\", \"/local_home/ag62216/var/dragonnet/result/ihdp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/local_home/ag62216/var/dragonnet/dat/ihdp/csv/dragonnet'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_base_dir = \"/local_home/ag62216/var/dragonnet/dat/ihdp/csv\"\n",
    "dragon='dragonnet'\n",
    "output_dir = os.path.join(data_base_dir, \"dragonnet\")\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dragon is dragonnet\n",
      "['/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_1.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_10.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_11.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_12.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_13.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_14.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_15.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_16.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_17.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_18.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_19.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_2.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_20.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_21.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_22.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_23.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_24.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_25.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_26.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_27.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_28.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_29.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_3.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_30.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_31.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_32.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_33.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_34.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_35.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_36.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_37.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_38.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_39.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_4.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_40.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_41.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_42.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_43.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_44.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_45.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_46.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_47.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_48.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_49.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_5.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_50.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_6.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_7.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_8.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_9.csv']\n"
     ]
    }
   ],
   "source": [
    "#run_ihdp(data_base_dir=data_base_dir, output_dir=output_dir, dragon='dragonnet')\n",
    "knob_loss=dragonnet_loss_binarycross\n",
    "ratio=1.\n",
    "\n",
    "print(\"the dragon is {}\".format(dragon))\n",
    "\n",
    "simulation_files = sorted(glob.glob(\"{}/*.csv\".format(data_base_dir)))\n",
    "\n",
    "print(simulation_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 /local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_1.csv\n",
      "simulation_output_dir: /local_home/ag62216/var/dragonnet/dat/ihdp/csv/dragonnet/1\n",
      "x:: <class 'numpy.ndarray'> (747, 25)\n",
      "[ 1.          0.          1.          0.          0.          0.\n",
      "  0.          1.          0.          1.          1.          1.\n",
      "  1.          0.          0.          0.          0.          0.\n",
      "  0.         -0.52860282 -0.3434545   1.12855393  0.16170253 -0.31660318\n",
      "  1.29521594]\n",
      "t:: <class 'numpy.ndarray'> (747, 1)\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "y:: <class 'numpy.ndarray'> (747, 1)\n",
      "[[5.59991629]\n",
      " [6.87585616]\n",
      " [2.99627271]\n",
      " [1.36620569]\n",
      " [1.96353814]\n",
      " [4.76209035]\n",
      " [6.59404386]\n",
      " [2.90823459]\n",
      " [2.13134649]\n",
      " [2.60232276]]\n"
     ]
    }
   ],
   "source": [
    "#for idx, simulation_file in enumerate(simulation_files):\n",
    "idx = 1\n",
    "simulation_file = simulation_files[0]\n",
    "print(idx, simulation_file)\n",
    "simulation_output_dir = os.path.join(output_dir, str(idx))\n",
    "print(\"simulation_output_dir:\",simulation_output_dir)\n",
    "\n",
    "os.makedirs(simulation_output_dir, exist_ok=True)\n",
    "\n",
    "x = load_and_format_covariates_ihdp(simulation_file)\n",
    "t, y, y_cf, mu_0, mu_1 = load_all_other_crap(simulation_file)\n",
    "np.savez_compressed(os.path.join(simulation_output_dir, \"simulation_outputs.npz\"),\n",
    "                    t=t, y=y, y_cf=y_cf, mu_0=mu_0, mu_1=mu_1)\n",
    "\n",
    "print(\"x::\",type(x),x.shape)\n",
    "print(x[0])\n",
    "print(\"t::\",type(t),t.shape)\n",
    "print(t[0:10])\n",
    "print(\"y::\",type(y),y.shape)\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is targeted regularization: True\n",
      "I am here making dragonnet\n",
      "WARNING:tensorflow:AutoGraph could not transform <function make_tarreg_loss.<locals>.tarreg_ATE_unbounded_domain_loss at 0x7f81881571f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function make_tarreg_loss.<locals>.tarreg_ATE_unbounded_domain_loss at 0x7f81881571f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "***************************** elapsed_time is:  10.62252140045166\n",
      "TEST:: average propensity for treated: 0.2091255933046341 and untreated: 0.16319815814495087\n",
      "TRAIN:: average propensity for treated: 0.2397436946630478 and untreated: 0.16083697974681854\n"
     ]
    }
   ],
   "source": [
    "#for is_targeted_regularization in [True, False]:\n",
    "is_targeted_regularization = True\n",
    "print(\"Is targeted regularization: {}\".format(is_targeted_regularization))\n",
    "\n",
    "test_outputs, train_output = train_and_predict_dragons(t, y, x,\n",
    "                                                       targeted_regularization=is_targeted_regularization,\n",
    "                                                       output_dir=simulation_output_dir,\n",
    "                                                       knob_loss=knob_loss, ratio=ratio, dragon=dragon,\n",
    "                                                       val_split=0.2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here making dragonnet\n",
      "WARNING:tensorflow:AutoGraph could not transform <function make_tarreg_loss.<locals>.tarreg_ATE_unbounded_domain_loss at 0x7f8188157670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function make_tarreg_loss.<locals>.tarreg_ATE_unbounded_domain_loss at 0x7f8188157670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "***************************** elapsed_time is:  12.548786640167236\n",
      "TEST:: average propensity for treated: 0.2078229784965515 and untreated: 0.16515076160430908\n",
      "TRAIN:: average propensity for treated: 0.233172208070755 and untreated: 0.1632821410894394\n"
     ]
    }
   ],
   "source": [
    "test_outputs, train_output = train_and_predict_dragons(t, y, x,\n",
    "                                                       targeted_regularization=is_targeted_regularization,\n",
    "                                                       output_dir=simulation_output_dir,\n",
    "                                                       knob_loss=knob_loss, ratio=ratio, dragon=dragon,\n",
    "                                                       val_split=0.2, batch_size=747)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "def load_data(knob='default', replication=1, model='baseline', train_test='test'):\n",
    "    \"\"\"\n",
    "    loading train test experiment results\n",
    "    \"\"\"\n",
    "\n",
    "    #file_path = '../../result/{}/'.format(knob)\n",
    "    file_path = '../../result/ihdp/{}/'.format(knob)\n",
    "    data = load(file_path + '{}/{}/0_replication_{}.npz'.format(replication, model, train_test))\n",
    "\n",
    "    return data['q_t0'].reshape(-1, 1), data['q_t1'].reshape(-1, 1), data['g'].reshape(-1, 1), \\\n",
    "           data['t'].reshape(-1, 1), data['y'].reshape(-1, 1), data['index'].reshape(-1, 1), data['eps'].reshape(-1, 1)\n",
    "\n",
    "def load_truth(replication, knob):\n",
    "    \"\"\"\n",
    "    loading ground truth data\n",
    "    \"\"\"\n",
    "\n",
    "    #file_path =    '../../result/{}/{}/simulation_outputs.npz'.format(knob, replication)\n",
    "    file_path = '../../result/ihdp/{}/{}/simulation_outputs.npz'.format(knob,replication)\n",
    "    data = load(file_path)\n",
    "    mu_0 = data['mu_0']\n",
    "    mu_1 = data['mu_1']\n",
    "\n",
    "    return mu_1, mu_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_t0, q_t1, g, t, y_dragon, index, eps = load_data('dragonnet', idx, 'targeted_regularization', 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.994109467110972"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = load_truth(idx, 'dragonnet')\n",
    "mu_1, mu_0 = a[index], b[index]\n",
    "truth = (mu_1 - mu_0).mean()\n",
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mse(x, y):\n",
    "    return np.mean(np.square(x-y))\n",
    "\n",
    "def truncate_by_g(attribute, g, level=0.01):\n",
    "    keep_these = np.logical_and(g >= level, g <= 1.-level)\n",
    "\n",
    "    return attribute[keep_these]\n",
    "\n",
    "def psi_tmle_cont_outcome(q_t0, q_t1, g, t, y, eps_hat=None, truncate_level=0.05):\n",
    "    q_t0, q_t1, g, t, y = truncate_all_by_g(q_t0, q_t1, g, t, y, truncate_level)\n",
    "\n",
    "\n",
    "    g_loss = mse(g, t)\n",
    "    h = t * (1.0/g) - (1.0-t) / (1.0 - g)\n",
    "    full_q = (1.0-t)*q_t0 + t*q_t1 # predictions from unperturbed model\n",
    "\n",
    "    if eps_hat is None:\n",
    "        eps_hat = np.sum(h*(y-full_q)) / np.sum(np.square(h))\n",
    "\n",
    "    def q1(t_cf):\n",
    "        h_cf = t_cf * (1.0 / g) - (1.0 - t_cf) / (1.0 - g)\n",
    "        full_q = (1.0 - t_cf) * q_t0 + t_cf * q_t1  # predictions from unperturbed model\n",
    "        return full_q + eps_hat * h_cf\n",
    "\n",
    "    ite = q1(np.ones_like(t)) - q1(np.zeros_like(t))\n",
    "    psi_tmle = np.mean(ite)\n",
    "\n",
    "    # standard deviation computation relies on asymptotic expansion of non-parametric estimator, see van der Laan and Rose p 96\n",
    "    ic = h*(y-q1(t)) + ite - psi_tmle\n",
    "    psi_tmle_std = np.std(ic) / np.sqrt(t.shape[0])\n",
    "    initial_loss = np.mean(np.square(full_q-y))\n",
    "    final_loss = np.mean(np.square(q1(t)-y))\n",
    "\n",
    "\n",
    "    return psi_tmle, psi_tmle_std, eps_hat, initial_loss, final_loss, g_loss\n",
    "\n",
    "def get_estimate(q_t0, q_t1, g, t, y_dragon, index, eps, truncate_level=0.01):\n",
    "    \"\"\"\n",
    "    getting the back door adjustment & TMLE estimation\n",
    "    \"\"\"\n",
    "\n",
    "    psi_n = psi_naive(q_t0, q_t1, g, t, y_dragon, truncate_level=truncate_level)\n",
    "    psi_tmle, psi_tmle_std, eps_hat, initial_loss, final_loss, g_loss = psi_tmle_cont_outcome(q_t0, q_t1, g, t,\n",
    "                                                                                              y_dragon,\n",
    "                                                                                              truncate_level=truncate_level)\n",
    "    return psi_n, psi_tmle, initial_loss, final_loss, g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.3489566,\n",
       " 5.316218071607363,\n",
       " 2.2387972842060155,\n",
       " 2.2386030737409697,\n",
       " 0.15999545604994775)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_n, psi_tmle, initial_loss, final_loss, g_loss = get_estimate(q_t0, q_t1, g, t, y_dragon, index, eps,truncate_level=0.01)\n",
    "psi_n, psi_tmle, initial_loss, final_loss, g_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3548471178194479, 0.3221086044963908)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = abs(truth - psi_n).mean()\n",
    "tmle_err = abs(truth - psi_tmle).mean()\n",
    "err , tmle_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
