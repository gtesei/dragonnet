{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e893dc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, TerminateOnNaN\n",
    "from idhp_data import *\n",
    "\n",
    "\n",
    "def _split_output(yt_hat, t, y, y_scaler, x, index):\n",
    "    q_t0 = y_scaler.inverse_transform(yt_hat[:, 0].copy())\n",
    "    q_t1 = y_scaler.inverse_transform(yt_hat[:, 1].copy())\n",
    "    g = yt_hat[:, 2].copy()\n",
    "\n",
    "    if yt_hat.shape[1] == 4:\n",
    "        eps = yt_hat[:, 3][0]\n",
    "    else:\n",
    "        eps = np.zeros_like(yt_hat[:, 2])\n",
    "\n",
    "    y = y_scaler.inverse_transform(y.copy())\n",
    "    var = \"average propensity for treated: {} and untreated: {}\".format(g[t.squeeze() == 1.].mean(),\n",
    "                                                                        g[t.squeeze() == 0.].mean())\n",
    "    print(var)\n",
    "\n",
    "    return {'q_t0': q_t0, 'q_t1': q_t1, 'g': g, 't': t, 'y': y, 'x': x, 'index': index, 'eps': eps}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b254ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_dragons(t, y_unscaled, x, targeted_regularization=True, output_dir='',\n",
    "                              knob_loss=dragonnet_loss_binarycross, ratio=1., dragon='', val_split=0.2, batch_size=64):\n",
    "    verbose = 0\n",
    "    y_scaler = StandardScaler().fit(y_unscaled)\n",
    "    y = y_scaler.transform(y_unscaled)\n",
    "    train_outputs = []\n",
    "    test_outputs = []\n",
    "\n",
    "    if dragon == 'tarnet':\n",
    "        dragonnet = make_tarnet(x.shape[1], 0.01)\n",
    "\n",
    "    elif dragon == 'dragonnet':\n",
    "        print(\"I am here making dragonnet\")\n",
    "        dragonnet = make_dragonnet(x.shape[1], 0.01)\n",
    "\n",
    "    metrics = [regression_loss, binary_classification_loss, treatment_accuracy, track_epsilon]\n",
    "\n",
    "    if targeted_regularization:\n",
    "        loss = make_tarreg_loss(ratio=ratio, dragonnet_loss=knob_loss)\n",
    "    else:\n",
    "        loss = knob_loss\n",
    "\n",
    "    # for reporducing the IHDP experimemt\n",
    "\n",
    "    i = 0\n",
    "    #tf.random.set_random_seed(i)\n",
    "    tf.random.set_seed(i)\n",
    "    np.random.seed(i)\n",
    "    #print(\"x.shape::\",np.arange(x.shape[0]))\n",
    "    train_index, test_index = train_test_split(np.arange(x.shape[0]), test_size=val_split, random_state=1)\n",
    "    #test_index = train_index\n",
    "\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    t_train, t_test = t[train_index], t[test_index]\n",
    "\n",
    "    yt_train = np.concatenate([y_train, t_train], 1)\n",
    "\n",
    "    import time;\n",
    "    start_time = time.time()\n",
    "\n",
    "    dragonnet.compile(\n",
    "        optimizer=Adam(lr=1e-3),\n",
    "        loss=loss, metrics=metrics)\n",
    "\n",
    "    adam_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=2, min_delta=0.),\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=1e-8, cooldown=0, min_lr=0)\n",
    "\n",
    "    ]\n",
    "\n",
    "    dragonnet.fit(x_train, yt_train, callbacks=adam_callbacks,\n",
    "                  validation_split=val_split,\n",
    "                  epochs=100,\n",
    "                  batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    sgd_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0.),\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0., cooldown=0, min_lr=0)\n",
    "    ]\n",
    "\n",
    "    sgd_lr = 1e-5\n",
    "    momentum = 0.9\n",
    "    dragonnet.compile(optimizer=SGD(lr=sgd_lr, momentum=momentum, nesterov=True), loss=loss,\n",
    "                      metrics=metrics)\n",
    "    dragonnet.fit(x_train, yt_train, callbacks=sgd_callbacks,\n",
    "                  validation_split=val_split,\n",
    "                  epochs=300,\n",
    "                  batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"***************************** elapsed_time is: \", elapsed_time)\n",
    "\n",
    "    yt_hat_test = dragonnet.predict(x_test)\n",
    "    yt_hat_train = dragonnet.predict(x_train)\n",
    "\n",
    "    test_outputs += [_split_output(yt_hat_test, t_test, y_test, y_scaler, x_test, test_index)]\n",
    "    train_outputs += [_split_output(yt_hat_train, t_train, y_train, y_scaler, x_train, train_index)]\n",
    "    K.clear_session()\n",
    "\n",
    "    return test_outputs, train_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56fc1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn_knob(\"/local_home/ag62216/var/dragonnet/dat/ihdp/csv\", \"dragonnet\", \"/local_home/ag62216/var/dragonnet/result/ihdp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00615566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/local_home/ag62216/var/dragonnet/dat/ihdp/csv/dragonnet'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_base_dir = \"/local_home/ag62216/var/dragonnet/dat/ihdp/csv\"\n",
    "dragon='dragonnet'\n",
    "output_dir = os.path.join(data_base_dir, \"dragonnet\")\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "540f371a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dragon is dragonnet\n",
      "['/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_1.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_10.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_11.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_12.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_13.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_14.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_15.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_16.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_17.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_18.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_19.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_2.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_20.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_21.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_22.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_23.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_24.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_25.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_26.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_27.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_28.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_29.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_3.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_30.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_31.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_32.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_33.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_34.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_35.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_36.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_37.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_38.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_39.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_4.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_40.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_41.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_42.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_43.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_44.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_45.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_46.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_47.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_48.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_49.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_5.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_50.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_6.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_7.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_8.csv', '/local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_9.csv']\n"
     ]
    }
   ],
   "source": [
    "#run_ihdp(data_base_dir=data_base_dir, output_dir=output_dir, dragon='dragonnet')\n",
    "knob_loss=dragonnet_loss_binarycross\n",
    "ratio=1.\n",
    "\n",
    "print(\"the dragon is {}\".format(dragon))\n",
    "\n",
    "simulation_files = sorted(glob.glob(\"{}/*.csv\".format(data_base_dir)))\n",
    "\n",
    "print(simulation_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "639cf2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /local_home/ag62216/var/dragonnet/dat/ihdp/csv/ihdp_npci_1.csv\n",
      "simulation_output_dir: /local_home/ag62216/var/dragonnet/dat/ihdp/csv/dragonnet/0\n",
      "x:: <class 'numpy.ndarray'> (747, 25)\n",
      "[ 1.          0.          1.          0.          0.          0.\n",
      "  0.          1.          0.          1.          1.          1.\n",
      "  1.          0.          0.          0.          0.          0.\n",
      "  0.         -0.52860282 -0.3434545   1.12855393  0.16170253 -0.31660318\n",
      "  1.29521594]\n",
      "t:: <class 'numpy.ndarray'> (747, 1)\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "y:: <class 'numpy.ndarray'> (747, 1)\n",
      "[[5.59991629]\n",
      " [6.87585616]\n",
      " [2.99627271]\n",
      " [1.36620569]\n",
      " [1.96353814]\n",
      " [4.76209035]\n",
      " [6.59404386]\n",
      " [2.90823459]\n",
      " [2.13134649]\n",
      " [2.60232276]]\n"
     ]
    }
   ],
   "source": [
    "#for idx, simulation_file in enumerate(simulation_files):\n",
    "idx = 0 \n",
    "simulation_file = simulation_files[0]\n",
    "print(idx, simulation_file)\n",
    "simulation_output_dir = os.path.join(output_dir, str(idx))\n",
    "print(\"simulation_output_dir:\",simulation_output_dir)\n",
    "\n",
    "os.makedirs(simulation_output_dir, exist_ok=True)\n",
    "\n",
    "x = load_and_format_covariates_ihdp(simulation_file)\n",
    "t, y, y_cf, mu_0, mu_1 = load_all_other_crap(simulation_file)\n",
    "np.savez_compressed(os.path.join(simulation_output_dir, \"simulation_outputs.npz\"),\n",
    "                    t=t, y=y, y_cf=y_cf, mu_0=mu_0, mu_1=mu_1)\n",
    "\n",
    "print(\"x::\",type(x),x.shape)\n",
    "print(x[0])\n",
    "print(\"t::\",type(t),t.shape)\n",
    "print(t[0:10])\n",
    "print(\"y::\",type(y),y.shape)\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d7ca4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is targeted regularization: True\n",
      "I am here making dragonnet\n",
      "WARNING:tensorflow:AutoGraph could not transform <function make_tarreg_loss.<locals>.tarreg_ATE_unbounded_domain_loss at 0x7f4db0440280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function make_tarreg_loss.<locals>.tarreg_ATE_unbounded_domain_loss at 0x7f4db0440280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "***************************** elapsed_time is:  8.488697290420532\n",
      "average propensity for treated: 0.2091255933046341 and untreated: 0.16319814324378967\n",
      "average propensity for treated: 0.2397436946630478 and untreated: 0.16083696484565735\n"
     ]
    }
   ],
   "source": [
    "#for is_targeted_regularization in [True, False]:\n",
    "is_targeted_regularization = True \n",
    "print(\"Is targeted regularization: {}\".format(is_targeted_regularization))\n",
    "\n",
    "test_outputs, train_output = train_and_predict_dragons(t, y, x,\n",
    "                                                       targeted_regularization=is_targeted_regularization,\n",
    "                                                       output_dir=simulation_output_dir,\n",
    "                                                       knob_loss=knob_loss, ratio=ratio, dragon=dragon,\n",
    "                                                       val_split=0.2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8623a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here making dragonnet\n",
      "WARNING:tensorflow:AutoGraph could not transform <function make_tarreg_loss.<locals>.tarreg_ATE_unbounded_domain_loss at 0x7f4db012d5e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function make_tarreg_loss.<locals>.tarreg_ATE_unbounded_domain_loss at 0x7f4db012d5e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "test_outputs, train_output = train_and_predict_dragons(t, y, x,\n",
    "                                                       targeted_regularization=is_targeted_regularization,\n",
    "                                                       output_dir=simulation_output_dir,\n",
    "                                                       knob_loss=knob_loss, ratio=ratio, dragon=dragon,\n",
    "                                                       val_split=0.2, batch_size=747)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf96032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369edd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
